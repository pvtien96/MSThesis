\BOOKMARK [0][-]{chapter.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.1.1}{Overview of object recognition and tracking}{chapter.1}% 2
\BOOKMARK [2][-]{subsection.1.1.1}{Object recognition}{section.1.1}% 3
\BOOKMARK [2][-]{subsection.1.1.2}{Object tracking}{section.1.1}% 4
\BOOKMARK [1][-]{section.1.2}{Context and scope of the thesis}{chapter.1}% 5
\BOOKMARK [2][-]{subsection.1.2.1}{Egocentric vision}{section.1.2}% 6
\BOOKMARK [2][-]{subsection.1.2.2}{Background project}{section.1.2}% 7
\BOOKMARK [1][-]{section.1.3}{Related works}{chapter.1}% 8
\BOOKMARK [2][-]{subsection.1.3.1}{Video object recoginiton and tracking related works}{section.1.3}% 9
\BOOKMARK [2][-]{subsection.1.3.2}{Hand recognition related works}{section.1.3}% 10
\BOOKMARK [1][-]{section.1.4}{Problem formulation and assumptions}{chapter.1}% 11
\BOOKMARK [2][-]{subsection.1.4.1}{Objective}{section.1.4}% 12
\BOOKMARK [2][-]{subsection.1.4.2}{Contribution}{section.1.4}% 13
\BOOKMARK [2][-]{subsection.1.4.3}{Thesis outline}{section.1.4}% 14
\BOOKMARK [0][-]{chapter.2}{Methodology and Datasets}{}% 15
\BOOKMARK [1][-]{section.2.1}{Tracking by detection approach}{chapter.2}% 16
\BOOKMARK [1][-]{section.2.2}{Object detection and segmentation algorithms}{chapter.2}% 17
\BOOKMARK [2][-]{subsection.2.2.1}{RCNN model family}{section.2.2}% 18
\BOOKMARK [2][-]{subsection.2.2.2}{YOLO model family}{section.2.2}% 19
\BOOKMARK [1][-]{section.2.3}{Object tracking algorithms}{chapter.2}% 20
\BOOKMARK [2][-]{subsection.2.3.1}{SORT}{section.2.3}% 21
\BOOKMARK [2][-]{subsection.2.3.2}{DeepSORT}{section.2.3}% 22
\BOOKMARK [1][-]{section.2.4}{Egocentric vision datasets}{chapter.2}% 23
\BOOKMARK [2][-]{subsection.2.4.1}{GTEA family datatsets}{section.2.4}% 24
\BOOKMARK [2][-]{subsection.2.4.2}{EgoHands dataset}{section.2.4}% 25
\BOOKMARK [2][-]{subsection.2.4.3}{Micand32 dataset}{section.2.4}% 26
\BOOKMARK [0][-]{chapter.3}{Proposed Framework}{}% 27
\BOOKMARK [1][-]{section.3.1}{Proposed framework: tracking by detection}{chapter.3}% 28
\BOOKMARK [1][-]{section.3.2}{Training stage}{chapter.3}% 29
\BOOKMARK [2][-]{subsection.3.2.1}{Training detection and segmentation models}{section.3.2}% 30
\BOOKMARK [2][-]{subsection.3.2.2}{Training deep appearance descriptor for DeepSORT}{section.3.2}% 31
\BOOKMARK [1][-]{section.3.3}{Inference stage}{chapter.3}% 32
\BOOKMARK [1][-]{section.3.4}{Evaluation stage}{chapter.3}% 33
\BOOKMARK [0][-]{chapter.4}{Experiments}{}% 34
\BOOKMARK [1][-]{section.4.1}{Evaluation criteria}{chapter.4}% 35
\BOOKMARK [2][-]{subsection.4.1.1}{Object detection metrics}{section.4.1}% 36
\BOOKMARK [2][-]{subsection.4.1.2}{Object tracking metrics}{section.4.1}% 37
\BOOKMARK [1][-]{section.4.2}{Experimental results}{chapter.4}% 38
\BOOKMARK [2][-]{subsection.4.2.1}{Egocentric hand detection and segmentation result}{section.4.2}% 39
\BOOKMARK [2][-]{subsection.4.2.2}{Egocentric hand tracking result}{section.4.2}% 40
\BOOKMARK [1][-]{section.4.3}{Discussions}{chapter.4}% 41
\BOOKMARK [2][-]{subsection.4.3.1}{Object detection: tradeoff between accuracy and speed}{section.4.3}% 42
\BOOKMARK [2][-]{subsection.4.3.2}{The superiority of DeepSORT over SORT}{section.4.3}% 43
\BOOKMARK [2][-]{subsection.4.3.3}{Impact of detection method over tracking result}{section.4.3}% 44
\BOOKMARK [2][-]{subsection.4.3.4}{Complexsity of 4 types of patients's actions}{section.4.3}% 45
\BOOKMARK [2][-]{subsection.4.3.5}{Challenging cases}{section.4.3}% 46
\BOOKMARK [0][-]{chapter.5}{Conclusion}{}% 47
\BOOKMARK [1][-]{section.5.1}{Conclusion}{chapter.5}% 48
\BOOKMARK [2][-]{subsection.5.1.1}{Accomplishment}{section.5.1}% 49
\BOOKMARK [2][-]{subsection.5.1.2}{Drawback}{section.5.1}% 50
\BOOKMARK [1][-]{section.5.2}{Future works}{chapter.5}% 51
\BOOKMARK [0][-]{appendix.A}{Tables}{}% 52
\BOOKMARK [0][-]{appendix.B}{Figures}{}% 53
\BOOKMARK [1][-]{section.B.1}{Short-term tracking results on Micand32S}{appendix.B}% 54
\BOOKMARK [1][-]{section.B.2}{Long-term tracking results on Micand32E}{appendix.B}% 55
