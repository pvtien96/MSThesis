\chapter{Introduction}\label{chap:intro}

\section{Overview of object recognition and tracking}
\subsection{Object recognition}
In the past decade, modern computer vision technology based on AI and deep learning methods has undergone tremendous development. Today, it is used in applications such as image classification, facial recognition, object recognition in images, video analysis and classification, and image processing in robots and autonomous vehicles. Many computer vision tasks require intelligent segmentation of images to understand the content contained in the image and simplify the analysis of each part. Today's image segmentation technology uses deep learning models for computer vision to understand the real objects represented by each pixel in an image at a level that was unimaginable only ten years ago. Deep learning can learn patterns in visual input in order to predict the categories of objects that make up an image. The main deep learning architectures used for image processing are convolutional neural networks (CNN) or specific CNN frameworks such as AlexNet, VGG, Inception and ResNet. Generally, a deep learning model for computer vision is trained and executed on a dedicated graphics processing unit (GPU) to reduce calculation time.
Image segmentation is a key process in computer vision. It involves dividing the visual input into multiple parts to simplify image analysis. A segment represents an object or part of an object, and includes a set of pixels or "super pixels". Image segmentation divides pixels into larger parts, eliminating the need to treat a single pixel as an observation unit. Object recognition is divided into three levels. Image classification: to classify the entire image into categories. Object detection: to detect an object in the image and draw a rectangle around it. Object segmentation: to identify the various parts of the image and understand what objects they belong to. Segmentation lays the foundation for performing object detection and classification. There are two type of granularity within the segmentation process itself: semantic segmentation and instance segmentation. Semantic segmentation classifies all pixels of the image into meaningful object categories. These classes are "semantic interpretable" and correspond to the classes in the real world. For example, you can isolate all pixels related to cats and then color them green. This is also called dense prediction because it can predict the meaning of each pixel. Instance segmentation identifies each instance of each object in the image. It differs from semantic segmentation in that it does not classify each pixel. If there are three cars in the image, semantic segmentation will classify all cars into one instance, and instance segmentation will identify each car.
Image segmentation helps determine the relationship between objects and the context of objects in the image. Applications include face recognition, license plate recognition and satellite image analysis. Industries such as retail and fashion use image segmentation in image-based searches. Self-driving cars use it to understand their surroundings. Medical imaging-extract clinically relevant information from medical images. For example, radiologists can use machine learning to enhance analysis by segmenting images into different organs, tissue types, or disease symptoms. This reduces the time required to run diagnostic tests.
\subsection{Object tracking}
Video tracking is a field of computer vision that involves the positioning of moving objects in video. Video tracking has many applications in robotics, motion analysis and video surveillance. These applications usually need to track multiple objects at the same time, which is called multi-object tracking. In video object tracking, the goal is to locate one or more objects of interest or targets in each frame of the video. We usually locate the target by drawing the smallest rectangle or bounding box that contains the target. Video object tracking has a wide range of applications, for example, it can be used for autonomous driving, surveillance, human-computer interaction, motion analysis. There is a close relationship between tracking and detection. Detection includes locating one or more objects in a given image, and the goal of tracking is to locate these objects in the entire video and track which objects along the video frame. In order to track an object, you first need to provide an image of the object to the tracking algorithm, which can be done by a detection algorithm (detection-based tracker) or manually (detection free tracker). Tracking is very crucial because it may help to solve common challenging problems, such as lighting changes, motion blur, zoom ratio changes, occlusion when the target is partially or completely hidden by another object in the video for a period of time, poor image quality. There are two main approaches of tracking: single object tracking (SOT) and multiple object tracking (MOT). Almost all trackers with the best performance are trackers based on Siamese network or Correlation Filter (CF), combined with effective appearance models (CNN function, HOG, color name). In general challenges, most of the highest performance is based on CF trackers, their performance is better than Siam trackers, and for real-time challenges, most of the highest performance is Siam trackers, their performance is better than CF tracking Device. As the name suggests, in multi-object tracking, there are multiple objects to track. It is expected that the tracking algorithm will first determine the number of objects in each frame, and secondly, track the identity of each object from one frame to the next.
MOT is a challenging problem: ID switching is difficult to avoid, especially in crowded videos, and the nature and number of each frame are unknown. Therefore, the MOT algorithm strongly relies on the detection algorithm, and the detection algorithm itself is not perfect. A popular object tracking method is to use a method called tracking by detection. It uses object detection algorithms to detect objects present in the frame. These objects are then tracked by associating the objects in the current frame with the objects in the previous frame using a tracking algorithm. Having a reliable object detection method is crucial, because the tracking algorithm depends on the object detected in each frame. Recently, target detection algorithms based on convolutional neural networks have been able to achieve higher accuracy than traditional target detection methods. The improvement of object detection accuracy promotes the use of one-by-one detection and tracking method for multiple object tracking.
\section{Context and scope of the thesis}
\subsection{Egocentric vision}
Egocentric vision or first-person vision (FPV) is a subfield of computer vision, which requires the analysis of images and videos captured by a wearable camera, which is usually worn on the head or chest, and naturally approximates the camera wearer Vision. Therefore, the visual data captures the part of the scene where the user is focused on performing on-site tasks and provides a valuable perspective to understand the user's activities and their environment in the natural environment.
In recent years, the research community has adopted a self-centered perspective to solve computer vision challenges, such as activity recognition \cite{10.1109/ICCV.2011.6126269} and object detection \cite{5995444} that are traditionally considered to belong to the field of third-person vision. Since then, self-centered vision has been applied to more complex applications, including video summarization \cite{6247820} and social interaction analysis \cite{7780657}. It is worth noting that it has also been extended to the field of healthcare \cite{wearable}, in which static camera systems tend to struggle to a greater degree with regard to privacy issues \cite{6091176}. Ultimately, self-centered vision is associated with the field of augmented reality to enhance human-centered applications that provide help for specific tasks \cite{10.1145/3041164.3041185}, thereby enhancing human independence; applicable to human capabilities Damaged or reduced condition.
In the medical field, FPV can help build applications that aid people with dementia by recording the sensor carrier's day-to-day activities; In rehabilitation therapy, or motor support in the elderly, physicians are often interested in monitoring the patient's recovery progress through movements and daily activities such as arm lifts. , move the wrist in the grasp object. Currently, such monitoring tools are very limited in hospitals, mainly using the naked eye for observation. Through the automatic analysis and recognition of activities from the series of images captured by the carrier camera (FPV video), the treating doctor can identify and quantify the patient's progression for a therapeutic regimen. value accordingly.
In sports, the use of egocentric cameras is increasingly common. The egocentric systems don't just collect front-view imagery during the journey of speed sports such as cycling and skiing; but also supports analysis of accuracy in sports movements such as golf, basketball. One notable feature is that the FPV image sequence not only contains information about the ego-motion of the object itself, but also the interactive movements between the hand and the subject (shadow, golf club) simultaneously. In sports, the recording of movements at critical moments (ball contact point, hand movement direction) plays an important role in determining the athlete's success or failure.
In the field of teaching aids, virtual reality, the use of FPV techniques has brought remarkable results. It can be considered the role of the camera carried on the body as a sixth sense (six-sense). On the one hand, FPV assists in detecting the behavior (e.g. hand gestures) of the subject, on the other hand, the camera carries the observer or the affected person. From there, the system reacts in accordance with the user's requirements. One particular application is the interaction between an observed object and the person carrying a sensor while visiting a museum; The system can on the one hand detect the object that the visitor is interested in, on the other hand identify the behavior (gesture) that the visitor wants to interact with the system (to support details, or see objects from different perspectives). In educational applications, virtual reality is receiving more and more attention besides medical applications when developing Ego-centric vision systems.
\subsection{Background project}
This thesis falls under the scope of the project “Understanding Human Daily Life Activities from Egocentric Vision Using Advanced Technologies of Deep Learning” under the sponsorship of National Foundation for Science and Technology Development (NAFOSTED). Egocentric Vision is a carry-on image sensor system in which the person carrying the image sensor plays a central role. The problem of identifying the sensor carrier's activity plays an important role in the development of a number of medical applications such as dementia treatment and functional rehabilitation; in education and sports. The objective of the topic is to identify the sensor carrier's activities through analyzing two important factors: the role of hand gestures in interacting with objects; and the role of the environment in identifying sensor carrier activity. To achieve these goals, the topic will focus on solving the fundamental problems of egocentric-vision such as: condensing video from large data sources (up to millions of images / day), effectively exploiting relationships. system between the sensor carrier and the interactive object / object; and environment / context. Specifically, the project will conduct three main research contents as follows: (1) developing advanced deep learning techniques for the problem of segmentation, identification (context), hand operation under the perspective of sensor to carry; (2) How to effectively combine information sources (such as environmental factors, time, operating status); (3) Based on the proposed techniques, develop some medical applications; to support the elderly and the disabled in daily activities. The direct result of the topic is to form a strong research group on Egocentric-vision at MICA International Research Institute, Hanoi University of Science and Technology at the end. The outstanding feature of egocentric vision is that it provides a first-person perspective by placing a forward-wearing wearable camera on a person’s chest or head. This provides a unique human-centered view and is set in an optimal way to capture information that is arguably more relevant to the camera wearer \cite{6232429}. Naturally, this refers to the surrounding area and its content, usually composed of objects, hands, other people and the background of the scene. Being able to check the angle of the scene and collect all this information clearly, so that higher-level cues can be better inferred, such as quantifying the interaction between hands based on the proximity of the hands \cite{Recognition}, and performing objects based on the associated movement \cite{Bertasius}.

The role of this thesis in the project is to focus on detecting, segmenting and tracking human hand in egocentric videos. Accompanying the theoretical research result is the construction of a pipeline that allows training and integration of detection algorithms with tracking algorithms. The evaluation results on the test dataset of a combination of algorithms will be very useful in detecting and tracking human hands and are a premise for the next research stages.
\section{Related works}
\subsection{Video object recoginiton and tracking related works}
Densely Annotated Video Segmentation (DAVIS) challenge \cite{7780657} is a fairly famous public competition designed for the task of video object segmentation that has been going on for 4 years from 2016 to 2020. It is a benchmark dataset and evaluation methodology for video object segmentation that consists of fifty high quality, full HD video sequences, accompanied by densely annotated, pixel-accurate and per-frame ground truth segmentation. It provides a comprehensive analysis of several state-of-the-art segmentation approaches using three complementary metrics. These are: semi-supervised challenge, interactive challenge and unsupervised challenge with specific datasets, definitions, rules and evaluation metrics.

The popular Visual Object Tracking (VOT) challenges \cite{VOT_TPAMI} provides the visual tracking community with a precisely defined and repeatable way to compare short-term trackers, and provides a common platform for discussing evaluation and progress in the field of visual tracking. The goal of the challenge is to build a large database of benchmarks and organize seminars or similar activities to promote research on visual tracking.

Multiple Object Tracking (MOT) \cite{DBLP:journals/corr/MilanL0RS16} challenge is a well-known benchmark for multi-object tracking that collects a large variety of sequences and provides a framework for the standardized evaluation of multiple object tracking methods. Currently, the benchmark is focused on multiple people tracking, since pedestrians are by far the most studied object in the tracking community. The benchmark includes 2D, 3D and multi-camera challenges. The tracking evaluation is this thesis uses the devkit protocol of the MOT16 which provide several measures, from recall to precision to running time. 
\subsection{Hand recognition related works}
One of the first works on the understanding of egocentric activities focused on defining the inner message of the egocentric visual paradigm \cite{10.1109/ICCV.2011.6126269}. They use the extracted visual features to model the relationship between hands, objects, and actions to model activities, and demonstrate the mutual improvement provided by these relationships through bottom-up and top-down models. The project of MICA is also based on the concepts that hands and objects are essential for egocentric action recognition and video understanding.

In \cite{5995444}, the authors have developed a weakly supervised technique that can identify objects by sculpting out objects from large sequences of self-centered activities. In general, this is a difficult task, their algorithm uses domain-specific knowledge from a first-person perspective to make it feasible. The method will automatically segment the active object area, assign some areas to each object, and use semi-supervised learning to disseminate its information. They proved that this method can reliably compare active classes based on the usage patterns of objects.

Movement-based egocentric action recognition is described in \cite{7298625}. According to the motion and color-based features and trajectories extracted from the video frames, the interaction points of the hand and the object, the object, the head and the self-movement are declared as actions, but the position of the modeled hand is not paid special attention. \cite{7780578} describes another multi-modal approach to egocentric activity recognition. Here, the hand segmentation network, the object localization network and the network trained by the motion flow are combined to predict the action.

In \cite{DBLP:journals/corr/BertasiusPYS16}, an architecture based on two-stream visual segmentation was used to predict the interaction area between hands and objects in a video stream and model them as actions. In \cite{cai2016understanding}, the concept of hand-object interaction is further explored, and the object shape-related grasping related to modeling actions is detected. The end-to-end approach also includes \cite{DBLP:journals/corr/abs-1806-06157}, where in order to recognize actions, the network is trained on paired frames and optimized for action recognition, object segmentation and inter-frame object interaction, as well as their training targets associated with recurrent networks. This thesis also assumes that hands and objects are the basis of self-centered actions, but it emphasizes the need to clearly detect the areas and positions of hands and objects to recognize actions.
A lot of work has been done in the explicit exploration of opponents and objects and their temporal relationships. Hand detection, segmentation and recognition techniques were developed \cite{6619302} \cite{6910041} \cite{10.1016/j.cviu.2016.09.005}, and the results were used to model behaviors or activities. In this thesis, I rely on a single-frame object detector to detect hands. In \cite{7410583}, hand-based activity recognition from an egocentric perspective is discussed. Before inferring the activity, the EgoHands dataset and the egocentric hand detection and segmentation pipeline were developed. This is one of the manual works, showing the difference between relying on hand detection or segmentation and using manual label for activity classification. In the literature \cite{Recognition}, activity recognition based on egocentric hands is considered, in which the distance between the detected hands or the distance between the detected hands and the objects marked as activities is regarded as the feature of activity classification.
In \cite{9060114}, the egocentric human action recognition is solved by explicitly using the existence and location of the region of interest in the scene without further use of visual features. Their understanding that the human hand is very important in performing actions, and focused on its actions as the main clues to define actions is similar to this thesis, but I focus on comparison the affection of the detector (RCNN and YOLO family) and tracker (SORT \cite{DBLP:journals/corr/BewleyGORU16} or DeepSORT \cite{DBLP:journals/corr/WojkeBP17} ) on locating the hands and capturing its movement. Finally, in all the works above, the suggestion on the choice of a suitable methods for online tracking applications is unavailable.
\section{Problem formulation and assumptions}
\subsection{Objective}
\begin{table}
	\label{formulation}
	\begin{tabular}{|l|p{13cm}|}
		
		\hline 
		Input &  Sequence of frames from an egocentric surveillance video\\ 
		\hline 
		Output & Trajectories of the tracked hands, includes: \\
		& \tabitem The number of human hands in the current frame \\
		& \tabitem The location of each hand, represented by a bounding box for the detection algorithm, or a set of pixels for the segmentation algorithm \\
		& \tabitem Identity of each hand across the video
\\ 
		\hline 
	\end{tabular}
	\caption{Problem formulation.}
\end{table}
Table \ref{formulation} define the input and output of the problem in this thesis. The goal of this thesis is to solve the problem of detecting, segmenting and tracking human hand objects in the video from the first perspective. Along with researching, confirming the theory, and proposing a hand tracking by detection system, I built a software to detect and track human hands in videos from the first perspective. The tracking by detection approach uses the combination of a detector and a tracker as following:
\begin{itemize}
	\item Detector: Faster RCNN, Mask RCNN, MaskRCNN with region based, YOLOv3, YOLOv4 or Ground-truth
	\item Tracker: SORT or DeepSORT
\end{itemize}
The inference results on test dataset are used to analyze the effect of phase detection algorithm selection on hand tracking phase, and also the efficiency of the DeepSORT algorithm compared to the SORT algorithm in the tracking phase.
\subsection{Contribution}
The main contributions of this thesis are three-folds:
\begin{itemize}
	\item First, a hand detection, segmentation and tracking from egocentric vision pipeline is built. This platform contains many state-of-the-art detection algorithms in RCNN and YOLO families, and also 2 tracking algorithm SORT and DeepSORT. Other algorithms can be simply integrated into this framework.
	\item Second, a comparative evaluation of combination of a detector and a tracker is conducted and analyzed on both term of accuracy and performance. Consequence, the recommendation for choosing the method on egocentric hand tracking applications is given.
	\item Third, Micand32, a new egocentric hand detection, segmentation and tracking datasets is built during this thesis, accompanied with labeling, visualizing and evaluation tools.
	Part of this thesis is inherited from a part of my previous work \cite{tien}. Code in this thesis is open-source and will made available at \faicon{github} \url{https://github.com/pvtien96/Detectron2DeepSortPlus}.
\end{itemize}
\subsection{Thesis outline}
The thesis is structured into 5 chapters:
\begin{enumerate}
	\item \textbf{Introduction} summarize the overview of object recognition, including object classification, detection, segmentation and tracking in the wild and in the first persion vision (FPV). Related works are studied and dicussed. The thesis's scope, objective and contribution are then described.
	\item \textbf{Methodology and Datasets} presents the RCNN and YOLO family models; SORT and DeepSORT algorithms. This chapter analyse egocentric hand datasets incoporate GTEA family, EgoHand; and futhermore present a new egocentric hand tracking datasets Micand32 alongside with a semi-automatic annotator,the EHTA.
	\item \textbf{Proposed Framework} introduces the proposed architecture which contains 4 stage: data preparing stage, training stage, inference stage and evaluation stage. This chapter reports in detail techniques in training and tesing phase.
	\item \textbf{Experiments} conserves about the evaluation criteria and then enumerate overall and featured detection, segmentation result following the COCO dataset standard and tracking result of 11 aproaches on Micand32S and Micand32E following the MOT Challenge standard. 
	\item \textbf{Conclusion} terminates the thesis by considering the accomplishment, the drawback and also purpose some potential future works.
\end{enumerate}