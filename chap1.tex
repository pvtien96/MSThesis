\chapter{Introduction}\label{chap:intro}

\section{Overview of object recognition and tracking from video}
\subsection{Object recognition}
Object recognition aims at detecting the presence of an object in image and giving it a label that is the category to which it belongs. Objects of interest can be face, vehicle, hand, people, tree, tumors depending on applications such as face id, intelligent traffic system and autonomous vehicles, human machine interaction, health care, security or bio diversity, etc. Segmentation is fundamental that go further than object detection and classification by give a label to a pixel, not a bounding box. There are two types of segmentation: semantic segmentation and instance segmentation. Semantic segmentation classifies all pixels of an image into meaningful object categories. These categories are "semantically interpretable" and correspond to the classes in the real world. Semantic segmentation gives an unique label to two objects of the same category. This is called dense prediction because it can predict the meaning of each pixel. Instance segmentation otherwise gives every pixel belonging to an object instance a label.
Since the past decade, object detection and recognition as well as segmentation has achieved impressive performance thanks to the significant advances of AI and deep learning. Deep learning can learn patterns in visual input in order to predict not only the categories of objects in image but also the ones of pixels. Deep learning architectures used for object detection / recognition / segmentation are convolutional neural networks (CNN) or specific CNN frameworks such as AlexNet, VGG, Inception and ResNet.
\subsection{Object tracking}
Tracking objects in a video stream involves association of moving objects in consecutive video frames. In order to track an object, the target object requires to be firstly detected manually (detection free trackers) or automatically by detection algorithms (detection based tracker). Then tracking algorithm will associate detected objects to existing tracks by putting constraints on distance function between movement and appearance of the object with its previous instances. Besides, giving a complete trajectory of a moving object during time is very important for video analysis. Tracking is more helpful to solve some common challenges (e.g. as lighting changes, motion blur, zoom ratio changes, occlusion when the target is partially or completely hidden by another object in the video for a period of time, poor image quality) from that simple object detection often suffers from.
\\Almost proposed trackers until now based on Siamese network or Correlation Filter (CF), combined with effective appearance models (CNN, HOG). In challenges on object tracking task, most of the highest performance obtained with CF trackers. Their performances is better than Siam tracker. In term of computational time, Siam tracker's performance is better than CF. Depending on the context and application, tracking could be single object tracking (SOT) and multiple object tracking (MOT).
MOT is more challenging than SOT because ID switching is difficult to avoid, especially in crowded videos, the nature and number of objects in each frame are unknown. Therefore, MOT algorithms strongly rely on detection algorithms. Unfortunately, detection algorithms itself are not perfect. A popular object tracking method is to use a method called tracking by detection. It first apply object detection algorithms to detect objects in current frame. These objects are then tracked by associating the objects in the current frame with the objects in the previous frame using a tracking algorithm. Having a reliable object detection method is crucial, because the tracking algorithm depends on the objects detected in each frame. Recently, target detection algorithms based on convolutional neural networks have been able to achieve higher accuracy than traditional target detection methods. The improvement of object detection accuracy promotes the use of one-by-one detection and tracking method for multiple object tracking.
\section{Context and scope of the thesis}
\subsection{Egocentric vision}
Egocentric vision or first-person vision (FPV) is a subfield of computer vision, which requires the analysis of images and videos captured by a wearable camera, which is usually worn on the head or chest, and naturally approximates the camera wearer vision. Therefore, the visual data captures the part of the scene where the user is focused on performing on-site tasks and provides a valuable perspective to understand the user's activities and their environment in the natural environment.
In recent years, the research community has adopted a self-centered perspective to solve computer vision challenges, such as activity recognition \cite{10.1109/ICCV.2011.6126269} and object detection \cite{5995444} that are traditionally considered to belong to the field of third-person vision. Since then, self-centered vision has been applied to more complex applications, including video summarization \cite{6247820} and social interaction analysis \cite{7780657}. It is worth noting that it has also been extended to the field of healthcare \cite{wearable}, in which static camera systems tend to struggle to a greater degree with regard to privacy issues \cite{6091176}. Ultimately, self-centered vision is associated with the field of augmented reality to enhance human-centered applications that provide help for specific tasks \cite{10.1145/3041164.3041185}.
In the medical field, FPV can help to build applications that aid people with dementia by recording the sensor carrier's daily activities. In rehabilitation therapy, or motor support in the elderly, physicians are often interested in monitoring the patient's recovery progress through movements and daily activities such as arm lifts, movement of the wrist within grasp objects. Currently, monitoring are mainly using the naked eye for observation, automatic monitoring tools are very limited in hospitals. Through the automatic analysis and recognition of activities from the series of images captured by the carrier camera (FPV video), the treating doctor can identify and quantify the patient's progression for a therapeutic regimen value accordingly.
In sports, the use of egocentric cameras is increasingly common. The egocentric systems don't just collect front-view imagery during the journey of speed sports such as cycling and skiing but also supports analysis of accuracy in sports movements such as golf, basketball. One notable feature is that the FPV image sequence does not only contain information about the ego-motion of target itself, but also the interactive movements between the hand and the subject (shadow, golf club) simultaneously. In sports, the recording of movements at critical moments (ball contact point, hand movement direction) plays an important role in determining the athlete's success or failure.
In the field of teaching aids, virtual reality, the use of FPV techniques has brought remarkable results. It can be considered the role of the camera carried on the body as a sixth sense (six-sense). On the one hand, FPV assists in detecting the behavior (e.g. hand gestures) of the subject, on the other hand, the camera carries the observer or the affected person. Consequently, the system  reacts in accordance with the user's requirements. One particular application is the interaction between an observed object and the person carrying a sensor while visiting a museum. Not only the system can detect the object that the visitor is interested in but also identify the behavior (gesture) that the visitor wants to interact with the system (to support details, or see objects from different perspectives). In educational applications, virtual reality is receiving more and more attention besides medical applications when developing egocentric vision systems.
\subsection{Background project and motivation} \label{subsec:bg}
This thesis falls under the scope of the project “Understanding Human Daily Life Activities from Egocentric Vision Using Advanced Technologies of Deep Learning” funded by National Foundation for Science and Technology Development (NAFOSTED). The objective of this project is to analyze activities of human (elderly people or patients) in daily life or during rehabilitation through analyzing two important factors: the role of hand gestures in interacting with objects; and the role of the environment in identifying sensor carrier activity.
To achieve these goals, the project focuses on solving the fundamental problems of egocentric-vision such as: summarizing video from large data sources (up to millions of images per day), effectively exploiting relationships between the sensor carrier and the interactive object, between environment and context. Specifically, the project implements three main tasks: (1) developing advanced deep learning techniques for hand segmentation and activities recognition, (2) exploiting the correlation between different factors such as  environment condition, time execution and the way of execution of a certain exercise/activity; (3) deploying proposed techniques to evaluate the rehabilitation of patients after a period of training.
\\My thesis in the project focuses on detecting, segmenting and tracking human hand in egocentric videos. I first study existing deep learning based methods for object segmentation and tracking then propose a framework that takes consecutive frames as input, segments hand regions frame by frame then track the hands along frames. I then implement the framework which allows to flexibly integrate different CNN architectures for objects detection (i.e. YOLO \cite{DBLP:journals/corr/RedmonDGF15}, Faster CNN \cite{DBLP:journals/corr/RenHG015}, Mask R-CNN \cite{DBLP:journals/corr/HeGDG17}) as well as object tracking (i.e. SORT \cite{DBLP:journals/corr/BewleyGORU16}, DeepSORT \cite{DBLP:journals/corr/WojkeBP17}). To train CNN models, I develop a tool to speed up the tracking annotation using automatic segmentation results. Finally, I evaluate the developed framework on a hand dataset captured in the context of the project.
\section{Related works}
\subsection{Video object recognition and tracking challenges}
Densely Annotated Video Segmentation (DAVIS) challenge \cite{7780657} is a fairly famous public competition designed for the task of video object segmentation that has been going on for 4 years from 2016 to 2020. It is a benchmark dataset and evaluation methodology for video object segmentation that consists of fifty high quality, full HD video sequences, accompanied by densely annotated, pixel-accurate and per-frame ground truth segmentation. It provides a comprehensive analysis of several state-of-the-art segmentation approaches using three complementary metrics. These are: semi-supervised challenge, interactive challenge and unsupervised challenge with specific datasets, definitions, rules and evaluation metrics.
The popular Visual Object Tracking (VOT) challenges \cite{VOT_TPAMI} provides the visual tracking community with a precisely defined and repeatable way to compare short-term trackers, and provides a common platform for discussing evaluation and progress in the field of visual tracking. The goal of the challenge is to build a large database of benchmarks and organize seminars or similar activities to promote research on visual tracking.
Multiple Object Tracking (MOT) \cite{DBLP:journals/corr/MilanL0RS16} challenge is a well-known benchmark for multi-object tracking that collects a large variety of sequences and provides a framework for the standardized evaluation of multiple object tracking methods. Currently, the benchmark is focused on multiple people tracking, since pedestrians are by far the most studied object in the tracking community. The benchmark includes 2D, 3D and multi-camera challenges. The tracking evaluation is this thesis uses the devkit protocol of the MOT16 which provide several measures, from recall, precision to running time. 
\subsection{Hand gestures recognition related works}
One of the first works on the understanding of egocentric activities focused on defining the inner message of the egocentric visual paradigm \cite{10.1109/ICCV.2011.6126269}. They use the extracted visual features to model the relationship between hands, objects, and actions to model activities, and demonstrate the mutual improvement provided by these relationships through bottom-up and top-down models.
In \cite{5995444}, the authors have developed a weakly supervised technique that can identify objects by sculpting out objects from large sequences of self-centered activities. In general, this is a difficult task, their algorithm uses domain-specific knowledge from a first-person perspective to make it feasible. The method will automatically segment the active object area, assign some areas to each object, and use semi-supervised learning to disseminate its information. They proved that this method can reliably compare active classes based on the usage patterns of objects.

Movement-based egocentric action recognition is described in \cite{7298625}. According to the motion and color-based features and trajectories extracted from the video frames, the interaction points of the hand and the object, the head and the self-movement are declared as actions, but the position of the modeled hand is not paid special attention. \cite{7780578} describes another multi-modal approach to egocentric activity recognition. The hand segmentation network, the object localization network and the network trained by the motion flow are combined to predict the action.

In \cite{DBLP:journals/corr/BertasiusPYS16}, an architecture based on two-stream visual segmentation was used to predict the interaction area between hands and objects in a video stream and model them as actions. In \cite{cai2016understanding}, the concept of hand-object interaction is further explored, and the object shape-related grasping related to modeling actions is detected. The end-to-end approach also includes \cite{DBLP:journals/corr/abs-1806-06157}, where in order to recognize actions, the network is trained on paired frames and optimized for action recognition, object segmentation and inter-frame object interaction, as well as their training targets associated with recurrent networks. This work also assumes that hands and objects are the basis of self-centered actions, but it emphasizes the need to clearly detect the areas and positions of hands and objects to recognize actions.
A lot of works have been done in the explicit exploration of opponents and objects and their temporal relationships. Hand detection, segmentation and recognition techniques were developed \cite{6619302} \cite{6910041} \cite{10.1016/j.cviu.2016.09.005}, and the results were used to model behaviors or activities. In \cite{7410583}, hand-based activity recognition from an egocentric perspective is discussed. Before inferring the activity, the EgoHands dataset and the egocentric hand detection and segmentation pipeline were developed. This is one of the manual works, showing the difference between relying on hand detection or segmentation and using manual label for activity classification. In the literature \cite{Recognition}, activity recognition based on egocentric hands is considered, in which the distance between the detected hands or the distance between the detected hands and the objects marked as activities is considered as the feature of activity classification.
In \cite{9060114}, the egocentric human action recognition is solved by explicitly using the existence and location of the region of interest in the scene without further use of visual features. Their understanding that the human hand is very important in performing actions, and focused on its actions as the main clues to define actions. Finally, in all the works above, the suggestion on the choice of a suitable methods for online tracking applications is unavailable.
\section{Problem formulation and assumptions}
\subsection{Objective}
\begin{table}
	\caption{Problem formulation.}
	\label{formulation}
	\begin{tabular}{|l|p{13cm}|}
		
		\hline 
		Input &  Sequence of frames from an egocentric surveillance video\\ 
		\hline 
		Output & Trajectories of the tracked hands, includes: \\
		& \tabitem The number of human hands in the current frame \\
		& \tabitem The location of each hand, represented by a bounding box for the detection algorithm, or a set of pixels for the segmentation algorithm \\
		& \tabitem Identity of each hand across the video
\\ 
		\hline 
	\end{tabular}
\end{table}
Table \ref{formulation} defines the input and output of the problem in this thesis. The goal of this thesis is to solve the problem of detecting, segmenting and tracking human hand objects in the video from the first perspective. Along with researching, confirming the theory, and proposing a hand tracking by detection system, I built a module to detect and track human hands in videos from the first perspective. The tracking by detection approach uses the combination of a detector and a tracker as following:
\begin{itemize}
	\item Detector: Faster RCNN, Mask RCNN, MaskRCNN with region based, YOLOv3, YOLOv4 or Ground-truth
	\item Tracker: SORT or DeepSORT
\end{itemize}
The inference results on test dataset are used to analyze the effect of phase detection algorithm selection on hand tracking phase, and also the efficiency of the DeepSORT algorithm compared to the SORT algorithm in the tracking phase.
\subsection{Contribution}
The main contributions of this thesis are three-folds:
\begin{itemize}
	\item First, a framework for hand detection, segmentation and tracking from egocentric vision pipeline is proposed. This framework contains state-of-the-art detection algorithms in RCNN and YOLO families for object detection and segmentation, and two tracking algorithm SORT and DeepSORT for object tracking. Any other algorithms can be simply integrated into this framework.
	\item Second, a comparative evaluation of combination of a detector and a tracker is conducted and analyzed on both term of accuracy and performance. Consequently, the recommendation for choosing a suitable method for egocentric hand tracking applications is given.
	\item Third, Micand32, a new egocentric hand detection, segmentation and tracking datasets is built during this thesis, accompanied with labeling, visualizing and evaluation tools.
\end{itemize}
A part of this thesis is published in the Proceeding of the \(3^{rd}\) International Conference on Multimedia Analysis and Pattern Recognition \cite{tien}. The code developed in this thesis is made available at \faicon{github} \url{https://github.com/pvtien96/Detectron2DeepSortPlus}.
\subsection{Thesis outline}
The thesis is structured into 5 chapters:
\begin{enumerate}
	\item \textbf{Introduction} summarize the overview of object recognition, including object classification, detection, segmentation and tracking in the wild and in the first persion vision (FPV). Related works are studied and dicussed. The thesis's scope, objective and contribution are then described.
	\item \textbf{Methodology and Datasets} presents the RCNN and YOLO family models; SORT and DeepSORT algorithms. This chapter analyse egocentric hand datasets incoporate GTEA family, EgoHand; and futhermore present a new egocentric hand tracking datasets Micand32 alongside with a semi-automatic annotator,the EHTA.
	\item \textbf{Proposed Framework} introduces the proposed architecture which contains 4 stage: data preparing stage, training stage, inference stage and evaluation stage. This chapter reports in detail techniques in training and tesing phase.
	\item \textbf{Experiments} conserves about the evaluation criteria and then enumerate overall and featured detection, segmentation result following the COCO dataset standard and tracking result of 11 aproaches on Micand32S and Micand32E following the MOT Challenge standard. 
	\item \textbf{Conclusion} terminates the thesis by considering the accomplishment, the drawback and also purpose some potential future works.
\end{enumerate}