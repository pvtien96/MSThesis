@article{DBLP:journals/corr/BewleyGORU16,
	author    = {Alex Bewley and
	ZongYuan Ge and
	Lionel Ott and
	Fabio Ramos and
	Ben Upcroft},
	title     = {Simple Online and Realtime Tracking},
	journal   = {CoRR},
	volume    = {abs/1602.00763},
	year      = {2016},
	url       = {http://arxiv.org/abs/1602.00763},
	archivePrefix = {arXiv},
	eprint    = {1602.00763},
	timestamp = {Mon, 13 Aug 2018 16:48:04 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/BewleyGORU16.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/WojkeBP17,
	author    = {Nicolai Wojke and
	Alex Bewley and
	Dietrich Paulus},
	title     = {Simple Online and Realtime Tracking with a Deep Association Metric},
	journal   = {CoRR},
	volume    = {abs/1703.07402},
	year      = {2017},
	url       = {http://arxiv.org/abs/1703.07402},
	archivePrefix = {arXiv},
	eprint    = {1703.07402},
	timestamp = {Mon, 13 Aug 2018 16:47:39 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/WojkeBP17.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/Girshick15,
	author    = {Ross B. Girshick},
	title     = {Fast {R-CNN}},
	journal   = {CoRR},
	volume    = {abs/1504.08083},
	year      = {2015},
	url       = {http://arxiv.org/abs/1504.08083},
	archivePrefix = {arXiv},
	eprint    = {1504.08083},
	timestamp = {Mon, 13 Aug 2018 16:49:11 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/Girshick15.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/RenHG015,
	author    = {Shaoqing Ren and
	Kaiming He and
	Ross B. Girshick and
	Jian Sun},
	title     = {Faster {R-CNN:} Towards Real-Time Object Detection with Region Proposal
	Networks},
	journal   = {CoRR},
	volume    = {abs/1506.01497},
	year      = {2015},
	url       = {http://arxiv.org/abs/1506.01497},
	archivePrefix = {arXiv},
	eprint    = {1506.01497},
	timestamp = {Mon, 13 Aug 2018 16:46:02 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/RenHG015.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/HeGDG17,
	author    = {Kaiming He and
	Georgia Gkioxari and
	Piotr Doll{\'{a}}r and
	Ross B. Girshick},
	title     = {Mask {R-CNN}},
	journal   = {CoRR},
	volume    = {abs/1703.06870},
	year      = {2017},
	url       = {http://arxiv.org/abs/1703.06870},
	archivePrefix = {arXiv},
	eprint    = {1703.06870},
	timestamp = {Mon, 13 Aug 2018 16:46:36 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/HeGDG17.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/RedmonDGF15,
	author    = {Joseph Redmon and
	Santosh Kumar Divvala and
	Ross B. Girshick and
	Ali Farhadi},
	title     = {You Only Look Once: Unified, Real-Time Object Detection},
	journal   = {CoRR},
	volume    = {abs/1506.02640},
	year      = {2015},
	url       = {http://arxiv.org/abs/1506.02640},
	archivePrefix = {arXiv},
	eprint    = {1506.02640},
	timestamp = {Mon, 13 Aug 2018 16:48:08 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/RedmonDGF15.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/RedmonF16,
	author    = {Joseph Redmon and
	Ali Farhadi},
	title     = {{YOLO9000:} Better, Faster, Stronger},
	journal   = {CoRR},
	volume    = {abs/1612.08242},
	year      = {2016},
	url       = {http://arxiv.org/abs/1612.08242},
	archivePrefix = {arXiv},
	eprint    = {1612.08242},
	timestamp = {Mon, 13 Aug 2018 16:48:25 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/RedmonF16.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-1804-02767,
	author    = {Joseph Redmon and
	Ali Farhadi},
	title     = {YOLOv3: An Incremental Improvement},
	journal   = {CoRR},
	volume    = {abs/1804.02767},
	year      = {2018},
	url       = {http://arxiv.org/abs/1804.02767},
	archivePrefix = {arXiv},
	eprint    = {1804.02767},
	timestamp = {Mon, 13 Aug 2018 16:48:24 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1804-02767.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{bochkovskiy2020yolov4,
	title={YOLOv4: Optimal speed and accuracy of object detection. arXiv 2020},
	author={Bochkovskiy, A and Wang, CY and Liao, HYM},
	journal={arXiv preprint arXiv:2004.10934},
	pages={1--17},
	year={2020}
}
@article{DBLP:journals/corr/abs-1812-00442,
	author    = {Nicolai Wojke and
	Alex Bewley},
	title     = {Deep Cosine Metric Learning for Person Re-Identification},
	journal   = {CoRR},
	volume    = {abs/1812.00442},
	year      = {2018},
	url       = {http://arxiv.org/abs/1812.00442},
	archivePrefix = {arXiv},
	eprint    = {1812.00442},
	timestamp = {Tue, 01 Jan 2019 15:01:25 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1812-00442.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@INPROCEEDINGS{tien,
	author={V. -T. {Pham} and T. -L. {Le} and T. -H. {Tran} and T. P. {Nguyen}},
	booktitle={2020 International Conference on Multimedia Analysis and Pattern Recognition (MAPR)}, 
	title={Hand detection and segmentation using multimodal information from Kinect}, 
	year={2020},
	volume={},
	number={},
	pages={1-6},
	doi={10.1109/MAPR49794.2020.9237785}}
@inproceedings{10.1109/ICCV.2011.6126269,
	author = {Fathi, Alireza and Farhadi, Ali and Rehg, James M.},
	title = {Understanding Egocentric Activities},
	year = {2011},
	isbn = {9781457711015},
	publisher = {IEEE Computer Society},
	address = {USA},
	url = {https://doi.org/10.1109/ICCV.2011.6126269},
	doi = {10.1109/ICCV.2011.6126269},
	abstract = {We present a method to analyze daily activities, such as meal preparation, using video from an egocentric camera. Our method performs inference about activities, actions, hands, and objects. Daily activities are a challenging domain for activity recognition which are well-suited to an egocentric approach. In contrast to previous activity recognition methods, our approach does not require pre-trained detectors for objects and hands. Instead we demonstrate the ability to learn a hierarchical model of an activity by exploiting the consistent appearance of objects, hands, and actions that results from the egocentric context. We show that joint modeling of activities, actions, and objects leads to superior performance in comparison to the case where they are considered independently. We introduce a novel representation of actions based on object-hand interactions and experimentally demonstrate the superior performance of our representation in comparison to standard activity representations such as bag of words.},
	booktitle = {Proceedings of the 2011 International Conference on Computer Vision},
	pages = {407–414},
	numpages = {8},
	series = {ICCV '11}
}
@INPROCEEDINGS{5995444,  author={A. {Fathi} and X. {Ren} and J. M. {Rehg}},  booktitle={CVPR 2011},   title={Learning to recognize objects in egocentric activities},   year={2011},  volume={},  number={},  pages={3281-3288},  doi={10.1109/CVPR.2011.5995444}}
@INPROCEEDINGS{6247820,  author={Y. J. {Lee} and J. {Ghosh} and K. {Grauman}},  booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition},   title={Discovering important people and objects for egocentric video summarization},   year={2012},  volume={},  number={},  pages={1346-1353},  doi={10.1109/CVPR.2012.6247820}}
@INPROCEEDINGS{7780657,  author={R. {Yonetani} and K. M. {Kitani} and Y. {Sato}},  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},   title={Recognizing Micro-Actions and Reactions from Paired Egocentric Videos},   year={2016},  volume={},  number={},  pages={2629-2638},  doi={10.1109/CVPR.2016.288}}
@article{wearable,
	author = {Doherty, Aiden and Hodges, Steve and King, Abby and Smeaton, Alan and Berry, Emma and Moulin, Chris and Lindley, Siân and Kelly, Paul and Foster, Charles},
	year = {2013},
	month = {03},
	pages = {320-3},
	title = {Wearable cameras in health: The state of the art and future possibilities},
	volume = {44},
	journal = {American journal of preventive medicine},
	doi = {10.1016/j.amepre.2012.11.008}
}
@INPROCEEDINGS{6091176,  author={D. {Townsend} and F. {Knoefel} and R. {Goubran}},  booktitle={2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society},   title={Privacy versus autonomy: A tradeoff model for smart home monitoring technologies},   year={2011},  volume={},  number={},  pages={4749-4752},  doi={10.1109/IEMBS.2011.6091176}}
@inproceedings{10.1145/3041164.3041185,
	author = {Leelasawassuk, Teesid and Damen, Dima and Mayol-Cuevas, Walterio},
	title = {Automated Capture and Delivery of Assistive Task Guidance with an Eyewear Computer: The GlaciAR System},
	year = {2017},
	isbn = {9781450348355},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3041164.3041185},
	doi = {10.1145/3041164.3041185},
	abstract = {In this paper we describe and evaluate an assistive mixed reality system that aims to augment users in tasks by combining automated and unsupervised information collection with minimally invasive video guides. The result is a fully self-contained system that we call GlaciAR (Glass-enabled Contextual Interactions for Augmented Reality). It operates by extracting contextual interactions from observing users performing actions. GlaciAR is able to i) automatically determine moments of relevance based on a head motion attention model, ii) automatically produce video guidance information, iii) trigger these guides based on an object detection method, iv) learn without supervision from observing multiple users and v) operate fully on-board a current eyewear computer (Google Glass). We describe the components of GlaciAR together with user evaluations on three tasks. We see this work as a first step toward scaling up the notoriously difficult authoring problem in guidance systems and an exploration of enhancing user natural abilities via minimally invasive visual cues.},
	booktitle = {Proceedings of the 8th Augmented Human International Conference},
	articleno = {16},
	numpages = {9},
	keywords = {assistive computing, eyewear computing, task guidance, augmented reality},
	location = {Silicon Valley, California, USA},
	series = {AH '17}
}
@ARTICLE{6232429,  author={T. {Kanade} and M. {Hebert}},  journal={Proceedings of the IEEE},   title={First-Person Vision},   year={2012},  volume={100},  number={8},  pages={2442-2453},  doi={10.1109/JPROC.2012.2200554}}
@inbook{Recognition,
	author = {Nguyen, Thi and Nebel, Jean-Christophe and Flórez-Revuelta, Francisco},
	year = {2018},
	month = {06},
	pages = {390-398},
	title = {Recognition of Activities of Daily Living from Egocentric Videos Using Hands Detected by a Deep Convolutional Network},
	isbn = {978-3-319-92999-6},
	doi = {10.1007/978-3-319-93000-8_44}
}
@inproceedings{Bertasius,
	author = {Bertasius, Gedas and Park, Hyun and Yu, Stella and Shi, Jianbo},
	year = {2017},
	month = {07},
	pages = {},
	title = {First-Person Action-Object Detection with EgoNet},
	doi = {10.15607/RSS.2017.XIII.012}
}
@INPROCEEDINGS{7780454,  author={F. {Perazzi} and J. {Pont-Tuset} and B. {McWilliams} and L. {Van Gool} and M. {Gross} and A. {Sorkine-Hornung}},  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},   title={A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation},   year={2016},  volume={},  number={},  pages={724-732},  doi={10.1109/CVPR.2016.85}}
@article {VOT_TPAMI,
	author = {Matej Kristan and Jiri Matas and Ale\v{s} Leonardis and Tomas Vojir and Roman Pflugfelder and Gustavo Fernandez and Georg Nebehay and Fatih Porikli and Luka \v{C}ehovin},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
	title={A Novel Performance Evaluation Methodology for Single-Target Trackers},
	year={2016}, month={Nov}, volume={38}, number={11}, pages={2137-2155},
	doi={10.1109/TPAMI.2016.2516982}, ISSN={0162-8828}
}
@article{DBLP:journals/corr/MilanL0RS16,
	author    = {Anton Milan and
	Laura Leal{-}Taix{\'{e}} and
	Ian D. Reid and
	Stefan Roth and
	Konrad Schindler},
	title     = {{MOT16:} {A} Benchmark for Multi-Object Tracking},
	journal   = {CoRR},
	volume    = {abs/1603.00831},
	year      = {2016},
	url       = {http://arxiv.org/abs/1603.00831},
	archivePrefix = {arXiv},
	eprint    = {1603.00831},
	timestamp = {Mon, 13 Aug 2018 16:47:32 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/MilanL0RS16.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@INPROCEEDINGS{7298625,  author={Y. {Li} and  {Zhefan Ye} and J. M. {Rehg}},  booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},   title={Delving into egocentric actions},   year={2015},  volume={},  number={},  pages={287-295},  doi={10.1109/CVPR.2015.7298625}}
@INPROCEEDINGS{7780578,  author={M. {Ma} and H. {Fan} and K. M. {Kitani}},  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},   title={Going Deeper into First-Person Activity Recognition},   year={2016},  volume={},  number={},  pages={1894-1903},  doi={10.1109/CVPR.2016.209}}
@article{DBLP:journals/corr/BertasiusPYS16,
	author    = {Gedas Bertasius and
	Hyun Soo Park and
	Stella X. Yu and
	Jianbo Shi},
	title     = {First Person Action-Object Detection with EgoNet},
	journal   = {CoRR},
	volume    = {abs/1603.04908},
	year      = {2016},
	url       = {http://arxiv.org/abs/1603.04908},
	archivePrefix = {arXiv},
	eprint    = {1603.04908},
	timestamp = {Mon, 13 Aug 2018 16:48:58 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/BertasiusPYS16.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{cai2016understanding,
	title={Understanding Hand-Object Manipulation with Grasp Types and Object Attributes.},
	author={Cai, Minjie and Kitani, Kris M and Sato, Yoichi},
	booktitle={Robotics: Science and Systems},
	volume={3},
	year={2016},
	organization={Ann Arbor, Michigan;}
}
@article{DBLP:journals/corr/abs-1806-06157,
	author    = {Fabien Baradel and
	Natalia Neverova and
	Christian Wolf and
	Julien Mille and
	Greg Mori},
	title     = {Object Level Visual Reasoning in Videos},
	journal   = {CoRR},
	volume    = {abs/1806.06157},
	year      = {2018},
	url       = {http://arxiv.org/abs/1806.06157},
	archivePrefix = {arXiv},
	eprint    = {1806.06157},
	timestamp = {Mon, 13 Aug 2018 16:48:26 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1806-06157.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@INPROCEEDINGS{6619302,  author={C. {Li} and K. M. {Kitani}},  booktitle={2013 IEEE Conference on Computer Vision and Pattern Recognition},   title={Pixel-Level Hand Detection in Ego-centric Videos},   year={2013},  volume={},  number={},  pages={3570-3577},  doi={10.1109/CVPR.2013.458}}
@INPROCEEDINGS{6910041,  author={A. {Betancourt} and M. M. {Lopez} and C. S. {Regazzoni} and M. {Rauterberg}},  booktitle={2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops},   title={A Sequential Classifier for Hand Detection in the Framework of Egocentric Vision},   year={2014},  volume={},  number={},  pages={600-605},  doi={10.1109/CVPRW.2014.92}}
@article{10.1016/j.cviu.2016.09.005,
	author = {Betancourt, Alejandro and Morerio, Pietro and Barakova, Emilia and Marcenaro, Lucio and Rauterberg, Matthias and Regazzoni, Carlo},
	title = {Left/Right Hand Segmentation in Egocentric Videos},
	year = {2017},
	issue_date = {January 2017},
	publisher = {Elsevier Science Inc.},
	address = {USA},
	volume = {154},
	number = {C},
	issn = {1077-3142},
	url = {https://doi.org/10.1016/j.cviu.2016.09.005},
	doi = {10.1016/j.cviu.2016.09.005},
	abstract = {Wearable cameras allow people to record their daily activities from a user-centered (First Person Vision) perspective. Due to their favorable location, wearable cameras frequently capture the hands of the user, and may thus represent a promising user-machine interaction tool for different applications. Existent First Person Vision methods handle hand segmentation as a background-foreground problem, ignoring two important facts: i) hands are not a single "skin-like" moving element, but a pair of interacting cooperative entities, ii) close hand interactions may lead to hand-to-hand occlusions and, as a consequence, create a single hand-like segment. These facts complicate a proper understanding of hand movements and interactions. Our approach extends traditional background-foreground strategies, by including a hand-identification step (left-right) based on a Maxwell distribution of angle and position. Hand-to-hand occlusions are addressed by exploiting temporal superpixels. The experimental results show that, in addition to a reliable left/right hand-segmentation, our approach considerably improves the traditional background-foreground hand-segmentation.},
	journal = {Comput. Vis. Image Underst.},
	month = jan,
	pages = {73–81},
	numpages = {9},
	keywords = {Hand-identification, First person vision, Hand-segmentation, Egocentric vision}
}
@INPROCEEDINGS{7410583,  author={S. {Bambach} and S. {Lee} and D. J. {Crandall} and C. {Yu}},  booktitle={2015 IEEE International Conference on Computer Vision (ICCV)},   title={Lending A Hand: Detecting Hands and Recognizing Activities in Complex Egocentric Interactions},   year={2015},  volume={},  number={},  pages={1949-1957},  doi={10.1109/ICCV.2015.226}}
@INPROCEEDINGS{9060114,  author={G. {Kapidis} and R. {Poppe} and E. {Van Dam} and L. {Noldus} and R. {Veltkamp}},  booktitle={2019 IEEE SmartWorld, Ubiquitous Intelligence   Computing, Advanced   Trusted Computing, Scalable Computing   Communications, Cloud   Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)},   title={Egocentric Hand Track and Object-Based Human Action Recognition},   year={2019},  volume={},  number={},  pages={922-929},  doi={10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00185}}

@article{10.1145/3065386,
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	title = {ImageNet Classification with Deep Convolutional Neural Networks},
	year = {2017},
	issue_date = {June 2017},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {60},
	number = {6},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/3065386},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.},
	journal = {Commun. ACM},
	month = may,
	pages = {84–90},
	numpages = {7}
}
@article{DBLP:journals/corr/abs-1808-07256,
	author    = {Karanbir Singh Chahal and
	Kuntal Dey},
	title     = {A Survey of Modern Object Detection Literature using Deep Learning},
	journal   = {CoRR},
	volume    = {abs/1808.07256},
	year      = {2018},
	url       = {http://arxiv.org/abs/1808.07256},
	archivePrefix = {arXiv},
	eprint    = {1808.07256},
	timestamp = {Sun, 02 Sep 2018 15:01:53 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1808-07256.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-1708-02002,
	author    = {Tsung{-}Yi Lin and
	Priya Goyal and
	Ross B. Girshick and
	Kaiming He and
	Piotr Doll{\'{a}}r},
	title     = {Focal Loss for Dense Object Detection},
	journal   = {CoRR},
	volume    = {abs/1708.02002},
	year      = {2017},
	url       = {http://arxiv.org/abs/1708.02002},
	archivePrefix = {arXiv},
	eprint    = {1708.02002},
	timestamp = {Mon, 13 Aug 2018 16:46:12 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1708-02002.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/HuangRSZKFFWSG016,
	author    = {Jonathan Huang and
	Vivek Rathod and
	Chen Sun and
	Menglong Zhu and
	Anoop Korattikara and
	Alireza Fathi and
	Ian Fischer and
	Zbigniew Wojna and
	Yang Song and
	Sergio Guadarrama and
	Kevin Murphy},
	title     = {Speed/accuracy trade-offs for modern convolutional object detectors},
	journal   = {CoRR},
	volume    = {abs/1611.10012},
	year      = {2016},
	url       = {http://arxiv.org/abs/1611.10012},
	archivePrefix = {arXiv},
	eprint    = {1611.10012},
	timestamp = {Tue, 08 Sep 2020 16:29:29 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/HuangRSZKFFWSG016.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{10.1115/1.3662552,
	author = {Kalman, R. E.},
	title = "{A New Approach to Linear Filtering and Prediction Problems}",
	journal = {Journal of Basic Engineering},
	volume = {82},
	number = {1},
	pages = {35-45},
	year = {1960},
	month = {03},
	abstract = "{The classical filtering and prediction problem is re-examined using the Bode-Shannon representation of random processes and the “state-transition” method of analysis of dynamic systems. New results are: (1) The formulation and methods of solution of the problem apply without modification to stationary and nonstationary statistics and to growing-memory and infinite-memory filters. (2) A nonlinear difference (or differential) equation is derived for the covariance matrix of the optimal estimation error. From the solution of this equation the co-efficients of the difference (or differential) equation of the optimal linear filter are obtained without further calculations. (3) The filtering problem is shown to be the dual of the noise-free regulator problem. The new method developed here is applied to two well-known problems, confirming and extending earlier results. The discussion is largely self-contained and proceeds from first principles; basic concepts of the theory of random processes are reviewed in the Appendix.}",
	issn = {0021-9223},
	doi = {10.1115/1.3662552},
	url = {https://doi.org/10.1115/1.3662552},
	eprint = {https://asmedigitalcollection.asme.org/fluidsengineering/article-pdf/82/1/35/5518977/35\_1.pdf},
}
@article{doi:10.1002/nav.3800020109,
	author = {Kuhn, H. W.},
	title = {The Hungarian method for the assignment problem},
	journal = {Naval Research Logistics Quarterly},
	volume = {2},
	number = {1‐2},
	pages = {83-97},
	doi = {10.1002/nav.3800020109},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nav.3800020109},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/nav.3800020109},
	abstract = {Abstract Assuming that numerical scores are available for the performance of each of n persons on each of n jobs, the “assignment problem” is the quest for an assignment of persons to jobs so that the sum of the n scores so obtained is as large as possible. It is shown that ideas latent in the work of two Hungarian mathematicians may be exploited to yield a new method of solving this problem.},
	year = {1955}
}
@misc{wu2019detectron2,
	author =       {Yuxin Wu and Alexander Kirillov and Francisco Massa and
	Wan-Yen Lo and Ross Girshick},
	title =        {Detectron2},
	howpublished = {{\url{https://github.com/facebookresearch/detectron2}}},
	year =         {2019}
}
@misc{glenn_jocher_2020_4154370,
	author       = {Glenn Jocher and
	Alex Stoken and
	Jirka Borovec},
	title        = {{ultralytics/yolov5: v3.1 - Bug Fixes and 
	Performance Improvements}},
	howpublished = {{\url{https://github.com/ultralytics/yolov5}}},
	year         = 2020,
	publisher    = {Zenodo},
	version      = {v3.1},
	doi          = {10.5281/zenodo.4154370},
	url          = {https://doi.org/10.5281/zenodo.4154370}
}
@ARTICLE{9064606,  author={A. {Bandini} and J. {Zariffa}},  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},   title={Analysis of the hands in egocentric vision: A survey},   year={2020},  volume={},  number={},  pages={1-1},  doi={10.1109/TPAMI.2020.2986648}}
@INPROCEEDINGS{8909903,  author={X. {Hou} and Y. {Wang} and L. {Chau}},  booktitle={2019 16th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)},   title={Vehicle Tracking Using Deep SORT with Low Confidence Track Filtering},   year={2019},  volume={},  number={},  pages={1-6},  doi={10.1109/AVSS.2019.8909903}}
@INPROCEEDINGS{4651201,  author={O. {Shigeta} and S. {Kagami} and K. {Hashimoto}},  booktitle={2008 IEEE/RSJ International Conference on Intelligent Robots and Systems},   title={Identifying a moving object with an accelerometer in a camera view},   year={2008},  volume={},  number={},  pages={3872-3877},  doi={10.1109/IROS.2008.4651201}}
@book{hastie2009elements,
	title={The Elements of Statistical Learning: Data Mining, Inference, and Prediction},
	author={Hastie, T. and Tibshirani, R. and Friedman, J.H.},
	isbn={9780387848846},
	lccn={2008941148},
	series={Springer series in statistics},
	url={https://books.google.com.vn/books?id=eBSgoAEACAAJ},
	year={2009},
	publisher={Springer}
}
@inproceedings{fathigaze,
	author = {Fathi, Alireza and Li, Yin and Rehg, James},
	year = {2012},
	month = {10},
	pages = {314-327},
	title = {Learning to Recognize Daily Actions Using Gaze},
	volume = {7572},
	isbn = {9783642337178},
	doi = {10.1007/978-3-642-33718-5_23}
}
@article{li2020eye,
	title={In the Eye of the Beholder: Gaze and Actions in First Person Video},
	author={Li, Yin and Liu, Miao and Rehg, James M},
	journal={arXiv preprint arXiv:2006.00626},
	year={2020}
}
@inproceedings{10.1109/ICCV.2015.226,
	author = {Bambach, Sven and Lee, Stefan and Crandall, David J. and Yu, Chen},
	title = {Lending A Hand: Detecting Hands and Recognizing Activities in Complex Egocentric Interactions},
	year = {2015},
	isbn = {9781467383912},
	publisher = {IEEE Computer Society},
	address = {USA},
	url = {https://doi.org/10.1109/ICCV.2015.226},
	doi = {10.1109/ICCV.2015.226},
	abstract = {Hands appear very often in egocentric video, and their appearance and pose give important cues about what people are doing and what they are paying attention to. But existing work in hand detection has made strong assumptions that work well in only simple scenarios, such as with limited interaction with other people or in lab settings. We develop methods to locate and distinguish between hands in egocentric video using strong appearance models with Convolutional Neural Networks, and introduce a simple candidate region generation approach that outperforms existing techniques at a fraction of the computational cost. We show how these high-quality bounding boxes can be used to create accurate pixelwise hand regions, and as an application, we investigate the extent to which hand segmentation alone can distinguish between different activities. We evaluate these techniques on a new dataset of 48 first-person videos of people interacting in realistic environments, with pixel-level ground truth for over 15,000 hand instances.},
	booktitle = {Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV)},
	pages = {1949–1957},
	numpages = {9},
	series = {ICCV '15}
}
@inproceedings{10.1145/3343031.3350535,
	author = {Dutta, Abhishek and Zisserman, Andrew},
	title = {The VIA Annotation Software for Images, Audio and Video},
	year = {2019},
	isbn = {9781450368896},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3343031.3350535},
	doi = {10.1145/3343031.3350535},
	abstract = {In this paper, we introduce a simple and standalone manual annotation tool for images, audio and video: the VGG Image Annotator (VIA). This is a light weight, standalone and offline software package that does not require any installation or setup and runs solely in a web browser. The VIA software allows human annotators to define and describe spatial regions in images or video frames, and temporal segments in audio or video. These manual annotations can be exported to plain text data formats such as JSON and CSV and therefore are amenable to further processing by other software tools. VIA also supports collaborative annotation of a large dataset by a group of human annotators. The BSD open source license of this software allows it to be used in any academic project or commercial application.},
	booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},
	pages = {2276–2279},
	numpages = {4},
	keywords = {image annotation, audio/video annotation, manual annotation},
	location = {Nice, France},
	series = {MM '19}
}
@article{DBLP:journals/corr/abs-1808-01974,
	author    = {Chuanqi Tan and
	Fuchun Sun and
	Tao Kong and
	Wenchang Zhang and
	Chao Yang and
	Chunfang Liu},
	title     = {A Survey on Deep Transfer Learning},
	journal   = {CoRR},
	volume    = {abs/1808.01974},
	year      = {2018},
	url       = {http://arxiv.org/abs/1808.01974},
	archivePrefix = {arXiv},
	eprint    = {1808.01974},
	timestamp = {Sun, 02 Sep 2018 15:01:55 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1808-01974.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/GirshickDDM13,
	author    = {Ross B. Girshick and
	Jeff Donahue and
	Trevor Darrell and
	Jitendra Malik},
	title     = {Rich feature hierarchies for accurate object detection and semantic
	segmentation},
	journal   = {CoRR},
	volume    = {abs/1311.2524},
	year      = {2013},
	url       = {http://arxiv.org/abs/1311.2524},
	archivePrefix = {arXiv},
	eprint    = {1311.2524},
	timestamp = {Mon, 13 Aug 2018 16:48:09 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/GirshickDDM13.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@INPROCEEDINGS{6126456,  author={K. E. A. {van de Sande} and J. R. R. {Uijlings} and T. {Gevers} and A. W. M. {Smeulders}},  booktitle={2011 International Conference on Computer Vision},   title={Segmentation as selective search for object recognition},   year={2011},  volume={},  number={},  pages={1879-1886},  doi={10.1109/ICCV.2011.6126456}}
@article{Simonyan2015VeryDC,
	title={Very Deep Convolutional Networks for Large-Scale Image Recognition},
	author={K. Simonyan and Andrew Zisserman},
	journal={CoRR},
	year={2015},
	volume={abs/1409.1556}
}
@article{DBLP:journals/corr/RussakovskyDSKSMHKKBBF14,
	author    = {Olga Russakovsky and
	Jia Deng and
	Hao Su and
	Jonathan Krause and
	Sanjeev Satheesh and
	Sean Ma and
	Zhiheng Huang and
	Andrej Karpathy and
	Aditya Khosla and
	Michael S. Bernstein and
	Alexander C. Berg and
	Fei{-}Fei Li},
	title     = {ImageNet Large Scale Visual Recognition Challenge},
	journal   = {CoRR},
	volume    = {abs/1409.0575},
	year      = {2014},
	url       = {http://arxiv.org/abs/1409.0575},
	archivePrefix = {arXiv},
	eprint    = {1409.0575},
	timestamp = {Mon, 22 Jul 2019 14:55:31 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/RussakovskyDSKSMHKKBBF14.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/HeZR014,
	author    = {Kaiming He and
	Xiangyu Zhang and
	Shaoqing Ren and
	Jian Sun},
	title     = {Spatial Pyramid Pooling in Deep Convolutional Networks for Visual
	Recognition},
	journal   = {CoRR},
	volume    = {abs/1406.4729},
	year      = {2014},
	url       = {http://arxiv.org/abs/1406.4729},
	archivePrefix = {arXiv},
	eprint    = {1406.4729},
	timestamp = {Wed, 17 Apr 2019 17:23:45 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/HeZR014.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/LinDGHHB16,
	author    = {Tsung{-}Yi Lin and
	Piotr Doll{\'{a}}r and
	Ross B. Girshick and
	Kaiming He and
	Bharath Hariharan and
	Serge J. Belongie},
	title     = {Feature Pyramid Networks for Object Detection},
	journal   = {CoRR},
	volume    = {abs/1612.03144},
	year      = {2016},
	url       = {http://arxiv.org/abs/1612.03144},
	archivePrefix = {arXiv},
	eprint    = {1612.03144},
	timestamp = {Mon, 13 Aug 2018 16:48:50 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/LinDGHHB16.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/HeZRS15,
	author    = {Kaiming He and
	Xiangyu Zhang and
	Shaoqing Ren and
	Jian Sun},
	title     = {Deep Residual Learning for Image Recognition},
	journal   = {CoRR},
	volume    = {abs/1512.03385},
	year      = {2015},
	url       = {http://arxiv.org/abs/1512.03385},
	archivePrefix = {arXiv},
	eprint    = {1512.03385},
	timestamp = {Wed, 17 Apr 2019 17:23:45 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/HeZRS15.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@INPROCEEDINGS{8237586,  author={T. {Lin} and P. {Goyal} and R. {Girshick} and K. {He} and P. {Dollár}},  booktitle={2017 IEEE International Conference on Computer Vision (ICCV)},   title={Focal Loss for Dense Object Detection},   year={2017},  volume={},  number={},  pages={2999-3007},  doi={10.1109/ICCV.2017.324}}
@INPROCEEDINGS{9010985,  author={K. {Duan} and S. {Bai} and L. {Xie} and H. {Qi} and Q. {Huang} and Q. {Tian}},  booktitle={2019 IEEE/CVF International Conference on Computer Vision (ICCV)},   title={CenterNet: Keypoint Triplets for Object Detection},   year={2019},  volume={},  number={},  pages={6568-6577},  doi={10.1109/ICCV.2019.00667}}
@article{DBLP:journals/corr/LiuAESR15,
	author    = {Wei Liu and
	Dragomir Anguelov and
	Dumitru Erhan and
	Christian Szegedy and
	Scott E. Reed and
	Cheng{-}Yang Fu and
	Alexander C. Berg},
	title     = {{SSD:} Single Shot MultiBox Detector},
	journal   = {CoRR},
	volume    = {abs/1512.02325},
	year      = {2015},
	url       = {http://arxiv.org/abs/1512.02325},
	archivePrefix = {arXiv},
	eprint    = {1512.02325},
	timestamp = {Wed, 12 Feb 2020 08:32:49 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/LiuAESR15.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{tan1911efficientdet,
	title={Efficientdet: Scalable and efficient object detection. arXiv 2019},
	author={Tan, M and Pang, R and Le, QV},
	journal={arXiv preprint arXiv:1911.09070}
}
@INPROCEEDINGS{8711887,
	author={V. {Kiselev} and M. {Khlamov} and K. {Chuvilin}},
	booktitle={2019 24th Conference of Open Innovations Association (FRUCT)}, 
	title={Hand Gesture Recognition with Multiple Leap Motion Devices}, 
	year={2019},
	volume={},
	number={},
	pages={163-169},
	doi={10.23919/FRUCT.2019.8711887}}